{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "\n",
    "from sklearn import  ensemble, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cl():\n",
    "    dt = pd.read_csv('../train.csv' , encoding='big5')\n",
    "    dt = dt.drop(\"CUS_ID\" ,axis=1)\n",
    "    \n",
    "    ohi = pd.get_dummies(dt[\"GENDER\"])\n",
    "    dt = dt.drop(\"GENDER\",axis=1)\n",
    "    dt = pd.concat([ohi,dt],axis=1)\n",
    "    \n",
    "    ohi = pd.get_dummies(dt[\"AGE\"])\n",
    "    dt = dt.drop(\"AGE\",axis=1)\n",
    "    dt = pd.concat([ohi,dt],axis=1)\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    for i in dt.keys():    \n",
    "        for a in dt[i]:\n",
    "            if  a is not None and isinstance(a,str):\n",
    "                dt[i] = labelencoder.fit_transform(dt[i].astype(str))\n",
    "                break\n",
    "    \n",
    "    for i in dt.keys():\n",
    "        dt[i] = dt[i].fillna(dt[i].mean())\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt = data_cl()\n",
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(dt):\n",
    "    x = dt.drop(\"Y1\",axis=1)\n",
    "    y = dt[\"Y1\"]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "    lr=LogisticRegression()\n",
    "    lr.fit(train_X,train_y)\n",
    "\n",
    "\n",
    "    # 印出截距\n",
    "#     print(lr.intercept_)\n",
    "    # 印出係數\n",
    "    a = abs(lr.coef_)[0]\n",
    "    c =lr.coef_[0]\n",
    "    a = list(a)\n",
    "    c = list(c)\n",
    "    newa = list(c)\n",
    "    newa.sort()\n",
    "    x = []\n",
    "    for i in newa:\n",
    "        if abs(i) < 0.01:\n",
    "            x.append(dt.columns[c.index(i)])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense , Dropout , Activation\n",
    "\n",
    "\n",
    "def modeling(xt):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(1024 ,input_shape=(xt.shape[1],)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(32))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(16))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(4))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(2))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp(dt):\n",
    "    ft = dt.drop(dropx,axis=1)\n",
    "    fix_dt = ft.sample(frac = 1)\n",
    "\n",
    "    fraud_dt = fix_dt.loc[dt['Y1'] == 1]\n",
    "    non_fraud_dt = fix_dt.loc[dt['Y1'] == 0][:2000]\n",
    "\n",
    "    normal_distributed_dt = pd.concat([fraud_dt, non_fraud_dt])\n",
    "\n",
    "    new_dtf = normal_distributed_dt.sample(frac=1)\n",
    "    # new_dt = normal_distributed_dt.sample(frac=1, random_state=42)\n",
    "\n",
    "    new_dtf.shape\n",
    "    \n",
    "    return new_dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DSLAB\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4000/4000 [==============================] - 2s 446us/step - loss: 0.3847 - acc: 0.5288\n",
      "Epoch 2/200\n",
      "4000/4000 [==============================] - 1s 211us/step - loss: 0.3346 - acc: 0.5552\n",
      "Epoch 3/200\n",
      "4000/4000 [==============================] - 1s 204us/step - loss: 0.3085 - acc: 0.5685\n",
      "Epoch 4/200\n",
      "4000/4000 [==============================] - 1s 207us/step - loss: 0.2981 - acc: 0.5745\n",
      "Epoch 5/200\n",
      "4000/4000 [==============================] - 1s 211us/step - loss: 0.2772 - acc: 0.5970\n",
      "Epoch 6/200\n",
      "4000/4000 [==============================] - 1s 212us/step - loss: 0.2619 - acc: 0.6102\n",
      "Epoch 7/200\n",
      "4000/4000 [==============================] - 1s 218us/step - loss: 0.2573 - acc: 0.6018\n",
      "Epoch 8/200\n",
      "4000/4000 [==============================] - 1s 219us/step - loss: 0.2527 - acc: 0.6160\n",
      "Epoch 9/200\n",
      "4000/4000 [==============================] - 1s 203us/step - loss: 0.2426 - acc: 0.6388\n",
      "Epoch 10/200\n",
      "4000/4000 [==============================] - 1s 219us/step - loss: 0.2411 - acc: 0.6325\n",
      "Epoch 11/200\n",
      "4000/4000 [==============================] - 1s 229us/step - loss: 0.2393 - acc: 0.6340\n",
      "Epoch 12/200\n",
      "4000/4000 [==============================] - 1s 223us/step - loss: 0.2359 - acc: 0.6390\n",
      "Epoch 13/200\n",
      "4000/4000 [==============================] - 1s 223us/step - loss: 0.2326 - acc: 0.6522\n",
      "Epoch 14/200\n",
      "4000/4000 [==============================] - 1s 224us/step - loss: 0.2328 - acc: 0.6425\n",
      "Epoch 15/200\n",
      "4000/4000 [==============================] - 1s 220us/step - loss: 0.2305 - acc: 0.6530\n",
      "Epoch 16/200\n",
      "4000/4000 [==============================] - 1s 214us/step - loss: 0.2308 - acc: 0.6442\n",
      "Epoch 17/200\n",
      "4000/4000 [==============================] - 1s 242us/step - loss: 0.2271 - acc: 0.6513\n",
      "Epoch 18/200\n",
      "4000/4000 [==============================] - 1s 270us/step - loss: 0.2254 - acc: 0.6540\n",
      "Epoch 19/200\n",
      "4000/4000 [==============================] - 1s 268us/step - loss: 0.2279 - acc: 0.6557\n",
      "Epoch 20/200\n",
      "4000/4000 [==============================] - 1s 263us/step - loss: 0.2217 - acc: 0.6693\n",
      "Epoch 21/200\n",
      "4000/4000 [==============================] - 1s 265us/step - loss: 0.2170 - acc: 0.6775\n",
      "Epoch 22/200\n",
      "4000/4000 [==============================] - 1s 269us/step - loss: 0.2219 - acc: 0.6697\n",
      "Epoch 23/200\n",
      "4000/4000 [==============================] - 1s 260us/step - loss: 0.2197 - acc: 0.6683 0s - loss: 0.2163 \n",
      "Epoch 24/200\n",
      "4000/4000 [==============================] - 1s 254us/step - loss: 0.2179 - acc: 0.6765\n",
      "Epoch 25/200\n",
      "4000/4000 [==============================] - 1s 268us/step - loss: 0.2198 - acc: 0.6697\n",
      "Epoch 26/200\n",
      "4000/4000 [==============================] - 1s 264us/step - loss: 0.2133 - acc: 0.6793\n",
      "Epoch 27/200\n",
      "4000/4000 [==============================] - 1s 260us/step - loss: 0.2102 - acc: 0.6895\n",
      "Epoch 28/200\n",
      "4000/4000 [==============================] - 1s 260us/step - loss: 0.2136 - acc: 0.6812\n",
      "Epoch 29/200\n",
      "4000/4000 [==============================] - 1s 218us/step - loss: 0.2144 - acc: 0.6798\n",
      "Epoch 30/200\n",
      "4000/4000 [==============================] - 1s 233us/step - loss: 0.2134 - acc: 0.6783\n",
      "Epoch 31/200\n",
      "4000/4000 [==============================] - 1s 252us/step - loss: 0.2077 - acc: 0.6950\n",
      "Epoch 32/200\n",
      "4000/4000 [==============================] - 1s 234us/step - loss: 0.2053 - acc: 0.7023\n",
      "Epoch 33/200\n",
      "4000/4000 [==============================] - 1s 219us/step - loss: 0.2081 - acc: 0.6892\n",
      "Epoch 34/200\n",
      "4000/4000 [==============================] - 1s 213us/step - loss: 0.2075 - acc: 0.6920\n",
      "Epoch 35/200\n",
      "4000/4000 [==============================] - 1s 215us/step - loss: 0.2081 - acc: 0.6900\n",
      "Epoch 36/200\n",
      "4000/4000 [==============================] - 1s 211us/step - loss: 0.2077 - acc: 0.6877\n",
      "Epoch 37/200\n",
      "4000/4000 [==============================] - 1s 213us/step - loss: 0.2042 - acc: 0.7042\n",
      "Epoch 38/200\n",
      "4000/4000 [==============================] - 1s 217us/step - loss: 0.2066 - acc: 0.6950\n",
      "Epoch 39/200\n",
      "4000/4000 [==============================] - 1s 223us/step - loss: 0.2023 - acc: 0.7063 0s - loss: 0.2017 - acc: 0.706\n",
      "Epoch 40/200\n",
      "4000/4000 [==============================] - 1s 216us/step - loss: 0.2053 - acc: 0.6988\n",
      "Epoch 41/200\n",
      "4000/4000 [==============================] - 1s 218us/step - loss: 0.1990 - acc: 0.7160 0s - loss: 0.2011 - acc: 0\n",
      "Epoch 42/200\n",
      "4000/4000 [==============================] - 1s 214us/step - loss: 0.1997 - acc: 0.7143\n",
      "Epoch 43/200\n",
      "4000/4000 [==============================] - 1s 214us/step - loss: 0.2027 - acc: 0.7040\n",
      "Epoch 44/200\n",
      "4000/4000 [==============================] - 1s 210us/step - loss: 0.2025 - acc: 0.7053\n",
      "Epoch 45/200\n",
      "4000/4000 [==============================] - 1s 211us/step - loss: 0.2003 - acc: 0.7105\n",
      "Epoch 46/200\n",
      "4000/4000 [==============================] - 1s 222us/step - loss: 0.2091 - acc: 0.6888\n",
      "Epoch 47/200\n",
      "4000/4000 [==============================] - 1s 212us/step - loss: 0.2002 - acc: 0.7107\n",
      "Epoch 48/200\n",
      "4000/4000 [==============================] - 1s 216us/step - loss: 0.2147 - acc: 0.6855\n",
      "Epoch 49/200\n",
      "4000/4000 [==============================] - 1s 222us/step - loss: 0.2063 - acc: 0.6992\n",
      "Epoch 50/200\n",
      "4000/4000 [==============================] - 1s 215us/step - loss: 0.2118 - acc: 0.6835\n",
      "Epoch 51/200\n",
      "4000/4000 [==============================] - 1s 216us/step - loss: 0.2092 - acc: 0.6950\n",
      "Epoch 52/200\n",
      "4000/4000 [==============================] - 1s 211us/step - loss: 0.2043 - acc: 0.7067 0s - loss: 0.1979 -\n",
      "Epoch 53/200\n",
      "4000/4000 [==============================] - 1s 228us/step - loss: 0.1990 - acc: 0.7140 0s - loss: 0.2074\n",
      "Epoch 54/200\n",
      "4000/4000 [==============================] - 1s 222us/step - loss: 0.2004 - acc: 0.7130\n",
      "Epoch 55/200\n",
      "4000/4000 [==============================] - 1s 232us/step - loss: 0.1970 - acc: 0.7172\n",
      "Epoch 56/200\n",
      "4000/4000 [==============================] - 1s 215us/step - loss: 0.1966 - acc: 0.7160\n",
      "Epoch 57/200\n",
      "4000/4000 [==============================] - 1s 216us/step - loss: 0.1967 - acc: 0.7280\n",
      "Epoch 58/200\n",
      "4000/4000 [==============================] - 1s 215us/step - loss: 0.1952 - acc: 0.7270\n",
      "Epoch 59/200\n",
      "4000/4000 [==============================] - 1s 230us/step - loss: 0.1933 - acc: 0.7280\n",
      "Epoch 60/200\n",
      "4000/4000 [==============================] - 1s 240us/step - loss: 0.2007 - acc: 0.7140\n",
      "Epoch 61/200\n",
      "4000/4000 [==============================] - 1s 239us/step - loss: 0.1976 - acc: 0.7245\n",
      "Epoch 62/200\n",
      "4000/4000 [==============================] - 1s 225us/step - loss: 0.1989 - acc: 0.7200\n",
      "Epoch 63/200\n",
      "4000/4000 [==============================] - 1s 217us/step - loss: 0.1994 - acc: 0.7088\n",
      "Epoch 64/200\n",
      "4000/4000 [==============================] - 1s 219us/step - loss: 0.1926 - acc: 0.7310\n",
      "Epoch 65/200\n",
      "4000/4000 [==============================] - 1s 237us/step - loss: 0.1924 - acc: 0.7330\n",
      "Epoch 66/200\n",
      "4000/4000 [==============================] - 1s 237us/step - loss: 0.1918 - acc: 0.7287\n",
      "Epoch 67/200\n",
      "4000/4000 [==============================] - 1s 245us/step - loss: 0.1853 - acc: 0.7502\n",
      "Epoch 68/200\n",
      "4000/4000 [==============================] - 1s 304us/step - loss: 0.1863 - acc: 0.7452\n",
      "Epoch 69/200\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.1848 - acc: 0.7470\n",
      "Epoch 70/200\n",
      "4000/4000 [==============================] - 1s 288us/step - loss: 0.1857 - acc: 0.7430\n",
      "Epoch 71/200\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.1863 - acc: 0.7430\n",
      "Epoch 72/200\n",
      "4000/4000 [==============================] - 1s 266us/step - loss: 0.1964 - acc: 0.7225\n",
      "Epoch 73/200\n",
      "4000/4000 [==============================] - 1s 246us/step - loss: 0.1947 - acc: 0.7253\n",
      "Epoch 74/200\n",
      "4000/4000 [==============================] - 1s 234us/step - loss: 0.1873 - acc: 0.7410\n",
      "Epoch 75/200\n",
      "4000/4000 [==============================] - 1s 264us/step - loss: 0.1848 - acc: 0.7485\n",
      "Epoch 76/200\n",
      "2680/4000 [===================>..........] - ETA: 0s - loss: 0.1851 - acc: 0.743"
     ]
    }
   ],
   "source": [
    "fail = []\n",
    "acc_drop = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    dt = data_cl()\n",
    "    dropx = logreg(dt)\n",
    "    \n",
    "    xt = dt.drop(\"Y1\",axis=1)\n",
    "    xt = xt.drop(dropx,axis=1)\n",
    "    \n",
    "    model = modeling(xt)\n",
    "    \n",
    "    newdt = shp(dt)\n",
    "    x = newdt.drop(\"Y1\",axis=1)\n",
    "    y = newdt[\"Y1\"]\n",
    "    \n",
    "    model.fit(x,y, epochs=200, batch_size=134 ,verbose=1)\n",
    "    \n",
    "    test_y_predicted = model.predict_classes(xt)\n",
    "    p = model.evaluate(xt,dt[\"Y1\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if p[1] > 0.8:\n",
    "        print(dropx)\n",
    "        print(p)\n",
    "        acc_drop.append(dropx)\n",
    "        accfnc(p[1],dropx)\n",
    "        \n",
    "#         break\n",
    "    print(dropx)\n",
    "    print(p)\n",
    "    fail.append(p[1])\n",
    "    log(p[1],dropx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1e30ac19748>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (17,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../test.csv' , encoding='big5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_test_cl(dt):\n",
    "#     dt = pd.read_csv('../train.csv' , encoding='big5')\n",
    "    dt = dt.drop(\"CUS_ID\" ,axis=1)\n",
    "    \n",
    "    ohi = pd.get_dummies(dt[\"GENDER\"])\n",
    "    dt = dt.drop(\"GENDER\",axis=1)\n",
    "    dt = pd.concat([ohi,dt],axis=1)\n",
    "    \n",
    "    ohi = pd.get_dummies(dt[\"AGE\"])\n",
    "    dt = dt.drop(\"AGE\",axis=1)\n",
    "    dt = pd.concat([ohi,dt],axis=1)\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    for i in dt.keys():    \n",
    "        for a in dt[i]:\n",
    "            if  a is not None and isinstance(a,str):\n",
    "                dt[i] = labelencoder.fit_transform(dt[i].astype(str))\n",
    "                break\n",
    "    \n",
    "    for i in dt.keys():\n",
    "        dt[i] = dt[i].fillna(dt[i].mean())\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 129)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_test_cl(df)\n",
    "df = df.drop(dropx,axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predicted = model.predict_classes(df)\n",
    "test_y_predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = pd.read_csv('../submit_test.csv' , encoding='big5')\n",
    "fin[\"Ypred\"] = test_y_predicted\n",
    "fin.to_csv('submit_test'+str(name)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accfnc(name , dropx):\n",
    "    df = pd.read_csv('../test.csv' , encoding='big5')\n",
    "    df = data_test_cl(df)\n",
    "    df = df.drop(dropx,axis=1)\n",
    "    \n",
    "    test_y_predicted = model.predict_classes(df)\n",
    "    test_y_predicted.sum()\n",
    "    \n",
    "    fin = pd.read_csv('../submit_test.csv' , encoding='big5')\n",
    "    fin[\"Ypred\"] = test_y_predicted\n",
    "    \n",
    "    fin.to_csv('submit_test'+str(name)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(name , dropx):\n",
    "    ft ='log.txt'\n",
    "    with open(ft , 'a') as fil:\n",
    "         print(f\"acc:{name} % , drop:{dropx}\", file=fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
